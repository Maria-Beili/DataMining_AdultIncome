# DataMining_AdultIncome
ğŸ“Š Binary Classification of Annual Income â€“ Data Mining Project

ğŸ” Project Overview

This project, developed as part of the Data Mining course at the University of Mannheim, aims to predict whether an individual's annual income exceeds $50,000 based on demographic and occupational data. It leverages advanced preprocessing techniques, feature selection, and multiple machine learning models including Gradient Boosting, Random Forest, SVM, and NaÃ¯ve Bayes.

The final model achieves a weighted F1-score of 0.84, outperforming 88% of existing models on Kaggleâ€™s benchmark dataset.

ğŸ§  Technologies & Methods

Python, scikit-learn, pandas, matplotlib

Data Cleaning, One-Hot Encoding, Feature Engineering

Model Evaluation: Stratified Sampling, Cross-Validation

Hyperparameter Tuning: Randomized Search

Final Model: Gradient Boosting Classifier

ğŸ“ Dataset

Source: 1994 U.S. Census Income Data on Kaggle

Size: 31,947 entries, 12 attributes

Target: Income (>50K vs â‰¤50K)

ğŸ“ˆ Model Results

Gradient Boosting Classifier achieved the best performance.

Evaluation based on weighted F1-score to address class imbalance.

Error analysis and feature importance analysis included.

ğŸ“¦ How to Run

pip install -r requirements.txt
python model_pipeline.py

ğŸ“œ Authors

Miguel Mendes, JoÃ£o Ferreira, Maria Beili Mena, Paola Tomorri, Klea Hoxha, Sueda Sogutlu

ğŸ“Š ClasificaciÃ³n Binaria de Ingresos Anuales â€“ Proyecto de MinerÃ­a de Datos

ğŸ” DescripciÃ³n del Proyecto

Este proyecto, desarrollado para la asignatura de MinerÃ­a de Datos en la Universidad de Mannheim, tiene como objetivo predecir si el ingreso anual de una persona supera los $50,000 a partir de datos demogrÃ¡ficos y laborales. Se aplicaron tÃ©cnicas avanzadas de preprocesamiento, selecciÃ³n de caracterÃ­sticas, y se probaron varios modelos de aprendizaje automÃ¡tico, destacando Gradient Boosting, Random Forest, SVM y NaÃ¯ve Bayes.

El modelo final alcanzÃ³ un F1-score ponderado de 0.84, superando al 88% de los modelos de referencia en Kaggle.

ğŸ§  TecnologÃ­as y MÃ©todos

Python, scikit-learn, pandas, matplotlib

Limpieza de datos, codificaciÃ³n One-Hot, ingenierÃ­a de caracterÃ­sticas

EvaluaciÃ³n: muestreo estratificado, validaciÃ³n cruzada

Ajuste de hiperparÃ¡metros: Randomized Search

Modelo final: Gradient Boosting Classifier

ğŸ“ Conjunto de Datos

Fuente: Datos de ingresos del censo de EE.UU. de 1994 en Kaggle

TamaÃ±o: 31,947 registros, 12 atributos

Variable objetivo: Ingreso (>50K vs â‰¤50K)

ğŸ“ˆ Resultados del Modelo

El modelo Gradient Boosting obtuvo el mejor desempeÃ±o.

EvaluaciÃ³n basada en F1-score ponderado para abordar el desbalance de clases.

Incluye anÃ¡lisis de errores y anÃ¡lisis de importancia de caracterÃ­sticas.

ğŸ“¦ CÃ³mo Ejecutar

pip install -r requirements.txt
python model_pipeline.py

ğŸ“œ Autores

Miguel Mendes, JoÃ£o Ferreira, Maria Beili Mena, Paola Tomorri, Klea Hoxha, Sueda Sogutlu

